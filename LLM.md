# LLM Related References

## Reviews, Surveys, Perspectives
T. Xiao & J. Zhu (2025) _Foundations of Large Language Models_ [arXiv2501.09223](https://arxiv.org/pdf/2501.09223)

## Fine-Tuning LLMs
C. Wu et al. (2025) _Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization_ [arXiv2505.16737](https://arxiv.org/html/2505.16737v1)

## Hallucinations
O. Obeso et al. (2025) _Real-Time Detection of Hallucinated Entities in Long-Form Generation_ [arXiv2509.03531](https://arxiv.org/pdf/2509.03531)

A.T. Kalai et al. (2025) _Why Language Models Hallucinate_ [arXiv2509.04664](https://www.arxiv.org/pdf/2509.04664)

J. Yuan et al. (2025) _Give Me FP32 or Give Me Death?
Challenges and Solutions for Reproducible Reasoning_ [arXiv2506.09501](https://arxiv.org/pdf/2506.09501)

## Model Evaluations
E. Miller (2024) _Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations_ [arXiv:2411.00640](https://arxiv.org/pdf/2411.00640)
  - _A Statistical Approach to Model Evaluations_ [Anthropic Blog](https://www.anthropic.com/research/statistical-approach-to-model-evals)

## Memory Management
W. Kwon (2023) _Efficient Memory Management for Large Language Model Serving with PagedAttention_ [arXiv:2309.06180](https://arxiv.org/pdf/2309.06180)

## Mixture of Agents (MoA) Architecture
J. Wang, et al. (2024) _Mixture-of-Agents Enhances Large Language Model Capabilities_ [arXiv:2406.04692](https://arxiv.org/pdf/2406.04692); Mixture-of-Agents (MoA) [Github Repo](https://github.com/togethercomputer/moa)
