# LLM Related References

## Fine-Tuning LLMs
C. Wu et al. (2025) _Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization_ [arXiv2505.16737](https://arxiv.org/html/2505.16737v1)

## Hallucinations
O. Obeso et al. (2025) _Real-Time Detection of Hallucinated Entities in Long-Form Generation_ [arXiv2509.03531](https://arxiv.org/pdf/2509.03531)

A.T. Kalai et al. (2025) _Why Language Models Hallucinate_ [arXiv2509.04664](https://www.arxiv.org/pdf/2509.04664)
