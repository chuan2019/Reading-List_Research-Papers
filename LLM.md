# LLM Related References

## Reviews, Surveys, Perspectives
T. Xiao & J. Zhu (2025) _Foundations of Large Language Models_ [arXiv2501.09223](https://arxiv.org/pdf/2501.09223)

## Fine-Tuning LLMs
C. Wu et al. (2025) _Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization_ [arXiv2505.16737](https://arxiv.org/html/2505.16737v1)

Zhang et al. (2025) _Agentic Context Engineering: Evolving Contexts for Self-Improving
Language Models_ [arXiv:2510.04618](https://www.arxiv.org/pdf/2510.04618)



## Hallucinations
O. Obeso et al. (2025) _Real-Time Detection of Hallucinated Entities in Long-Form Generation_ [arXiv2509.03531](https://arxiv.org/pdf/2509.03531)

A.T. Kalai et al. (2025) _Why Language Models Hallucinate_ [arXiv2509.04664](https://www.arxiv.org/pdf/2509.04664)

J. Yuan et al. (2025) _Give Me FP32 or Give Me Death?
Challenges and Solutions for Reproducible Reasoning_ [arXiv2506.09501](https://arxiv.org/pdf/2506.09501)

## Model Evaluations
E. Miller (2024) _Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations_ [arXiv:2411.00640](https://arxiv.org/pdf/2411.00640)
  - _A Statistical Approach to Model Evaluations_ [Anthropic Blog](https://www.anthropic.com/research/statistical-approach-to-model-evals)

## Memory Management
W. Kwon (2023) _Efficient Memory Management for Large Language Model Serving with PagedAttention_ [arXiv:2309.06180](https://arxiv.org/pdf/2309.06180)

## Mixture of Agents (MoA) Architecture
J. Wang, et al. (2024) _Mixture-of-Agents Enhances Large Language Model Capabilities_ [arXiv:2406.04692](https://arxiv.org/pdf/2406.04692); Mixture-of-Agents (MoA) [Github Repo](https://github.com/togethercomputer/moa)

## Mixture of Experts (MoE)
W. Cai, et al. (2025) _A Survey on Mixture of Experts in Large Language Models_ **IEEE Transactions on Knowledge and Data Engineering** (TKDE) Vol. 37(7), pp. 3896-3915 [TKDE.2025.3554028](https://ieeexplore.ieee.org/document/10937907), [arXiv:2407.06204](https://arxiv.org/pdf/2407.06204)

N. Shazeer, et al. (2017) _Outrageously Large Neural Networks: the Sparsely-Gated Mixture-of-Experts Layer_ [arXiv:1701.06538](https://arxiv.org/pdf/1701.06538)

R.A. Jacobs, et al. (1991) _Adaptive Mixtures of Local Experts_ **Neural Computation** Vol. 3(1), pp. 79-87 [neco.1991.3.1.79](https://ieeexplore.ieee.org/document/6797059) [jjnh91.pdf](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)

M. Grootendorst (2024) _A Visual Guide to Mixture of Experts (MoE)_ [Exploring Language Models](https://newsletter.maartengrootendorst.com/) [Oct 07, 2024](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)

B. Pandit (2024) _What is Mixture of Experts (MoE)? How it Works, Use Cases & More_ [datacamp.blog](https://www.datacamp.com/blog/mixture-of-experts-moe)
